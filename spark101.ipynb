{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20080b01-1df0-4437-b03f-f250a3e46475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/10 08:11:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://brians-air.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x110a69130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import spark for python! \n",
    "import pyspark\n",
    "\n",
    "#create the spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0ac24d-7dad-484b-b9cb-35c7f1034969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb028ca-bf9e-43ea-bc11-e56dfe5838cc",
   "metadata": {},
   "source": [
    "## Create a spark data frame that contains your favorite programming languages.\n",
    "* The name of the column should be language\n",
    "* View the schema of the dataframe\n",
    "* Output the shape of the dataframe\n",
    "* Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7627896-1a04-4566-ac9d-3207bd2c673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    Python|\n",
      "|      Java|\n",
      "|JavaScript|\n",
      "|       C++|\n",
      "|      Ruby|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a list of your favorite programming languages\n",
    "favorite_languages = [\"Python\", \"Java\", \"JavaScript\", \"C++\", \"Ruby\"]\n",
    "\n",
    "# Create a DataFrame with the column name \"language\"\n",
    "df = spark.createDataFrame([(language,) for language in favorite_languages], [\"language\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209673ee-b4de-4be0-b21e-043cd55fc06e",
   "metadata": {},
   "source": [
    "## Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705a305f-def7-440d-991d-5ea45a47bf88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495ba742-faa7-4838-b2fd-114f1b04140b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data is already a DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the pandas DataFrame to Spark DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new column with the desired output message\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m                    F\u001b[38;5;241m.\u001b[39mconcat(F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39myear, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mmanufacturer, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mmodel, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has a \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mcyl, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cylinder engine.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:1224\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is already a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1227\u001b[0m     schema \u001b[38;5;241m=\u001b[39m cast(Union[AtomicType, StructType, \u001b[38;5;28mstr\u001b[39m], _parse_datatype_string(schema))\n",
      "\u001b[0;31mTypeError\u001b[0m: data is already a DataFrame"
     ]
    }
   ],
   "source": [
    "# Convert the pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(mpg)\n",
    "\n",
    "# Create a new column with the desired output message\n",
    "df = df.withColumn('output', \n",
    "                   F.concat(F.lit('The '), df.year, F.lit(' '), df.manufacturer, F.lit(' '), df.model, F.lit(' has a '), df.cyl, F.lit(' cylinder engine.')))\n",
    "\n",
    "# Transform the trans column to only contain either manual or auto\n",
    "df = df.withColumn('trans', F.when(df.trans.like('%manual%'), 'manual').otherwise('auto'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75a9694-8811-4238-be07-2296dea54779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b1669a-2318-4b8f-a446-284d307d639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|total_bill|tip |sex   |smoker|day|time  |size|tip_percentage    |\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|16.99     |1.01|Female|No    |Sun|Dinner|2   |5.9446733372572105|\n",
      "|10.34     |1.66|Male  |No    |Sun|Dinner|3   |16.054158607350097|\n",
      "|21.01     |3.5 |Male  |No    |Sun|Dinner|3   |16.658733936220845|\n",
      "|23.68     |3.31|Male  |No    |Sun|Dinner|2   |13.97804054054054 |\n",
      "|24.59     |3.61|Female|No    |Sun|Dinner|4   |14.680764538430255|\n",
      "|25.29     |4.71|Male  |No    |Sun|Dinner|4   |18.62396204033215 |\n",
      "|8.77      |2.0 |Male  |No    |Sun|Dinner|2   |22.80501710376283 |\n",
      "|26.88     |3.12|Male  |No    |Sun|Dinner|4   |11.607142857142858|\n",
      "|15.04     |1.96|Male  |No    |Sun|Dinner|2   |13.031914893617023|\n",
      "|14.78     |3.23|Male  |No    |Sun|Dinner|2   |21.853856562922868|\n",
      "|10.27     |1.71|Male  |No    |Sun|Dinner|2   |16.65043816942551 |\n",
      "|35.26     |5.0 |Female|No    |Sun|Dinner|4   |14.180374361883155|\n",
      "|15.42     |1.57|Male  |No    |Sun|Dinner|2   |10.181582360570687|\n",
      "|18.43     |3.0 |Male  |No    |Sun|Dinner|4   |16.277807921866522|\n",
      "|14.83     |3.02|Female|No    |Sun|Dinner|2   |20.364126770060686|\n",
      "|21.58     |3.92|Male  |No    |Sun|Dinner|2   |18.164967562557923|\n",
      "|10.33     |1.67|Female|No    |Sun|Dinner|3   |16.16650532429816 |\n",
      "|16.29     |3.71|Male  |No    |Sun|Dinner|3   |22.774708410067525|\n",
      "|16.97     |3.5 |Female|No    |Sun|Dinner|3   |20.624631703005306|\n",
      "|20.65     |3.35|Male  |No    |Sat|Dinner|3   |16.222760290556902|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Percentage of observations that are smokers: 38.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|sex   |smoker|avg_tip_percentage|\n",
      "+------+------+------------------+\n",
      "|Male  |No    |16.06687151291298 |\n",
      "|Female|No    |15.69209707691836 |\n",
      "|Male  |Yes   |15.277117520248513|\n",
      "|Female|Yes   |18.21503526994103 |\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the tips dataset from the pydataset library\n",
    "tips = data('tips')\n",
    "\n",
    "# Convert the pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(tips)\n",
    "\n",
    "# Calculate the percentage of observations that are smokers\n",
    "smokers_percentage = df.filter(df.smoker == 'Yes').count() / df.count() * 100\n",
    "\n",
    "# Create a column that contains the tip percentage\n",
    "df = df.withColumn('tip_percentage', F.col('tip') / F.col('total_bill') * 100)\n",
    "\n",
    "# Calculate the average tip percentage for each combination of sex and smoker\n",
    "avg_tip_percentage = df.groupby('sex', 'smoker').agg(F.avg('tip_percentage').alias('avg_tip_percentage'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(truncate=False)\n",
    "print(\"Percentage of observations that are smokers: {:.2f}%\".format(smokers_percentage))\n",
    "avg_tip_percentage.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f86d22-82e1-4c38-a735-2d33b6083272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
