{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20080b01-1df0-4437-b03f-f250a3e46475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/11 08:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/11 08:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://brians-air.localdomain:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111c4f130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import spark for python! \n",
    "import pyspark\n",
    "\n",
    "#create the spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0ac24d-7dad-484b-b9cb-35c7f1034969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb028ca-bf9e-43ea-bc11-e56dfe5838cc",
   "metadata": {},
   "source": [
    "## Create a spark data frame that contains your favorite programming languages.\n",
    "* The name of the column should be language\n",
    "* View the schema of the dataframe\n",
    "* Output the shape of the dataframe\n",
    "* Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7627896-1a04-4566-ac9d-3207bd2c673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    Python|\n",
      "|      Java|\n",
      "|JavaScript|\n",
      "|       C++|\n",
      "|      Ruby|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a list of your favorite programming languages\n",
    "favorite_languages = [\"Python\", \"Java\", \"JavaScript\", \"C++\", \"Ruby\"]\n",
    "\n",
    "# Create a DataFrame with the column name \"language\"\n",
    "df = spark.createDataFrame([(language,) for language in favorite_languages], [\"language\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209673ee-b4de-4be0-b21e-043cd55fc06e",
   "metadata": {},
   "source": [
    "## Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705a305f-def7-440d-991d-5ea45a47bf88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495ba742-faa7-4838-b2fd-114f1b04140b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data is already a DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the pandas DataFrame to Spark DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new column with the desired output message\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m                    F\u001b[38;5;241m.\u001b[39mconcat(F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39myear, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mmanufacturer, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mmodel, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has a \u001b[39m\u001b[38;5;124m'\u001b[39m), df\u001b[38;5;241m.\u001b[39mcyl, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cylinder engine.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:1224\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is already a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1227\u001b[0m     schema \u001b[38;5;241m=\u001b[39m cast(Union[AtomicType, StructType, \u001b[38;5;28mstr\u001b[39m], _parse_datatype_string(schema))\n",
      "\u001b[0;31mTypeError\u001b[0m: data is already a DataFrame"
     ]
    }
   ],
   "source": [
    "# Convert the pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(mpg)\n",
    "\n",
    "# Create a new column with the desired output message\n",
    "df = df.withColumn('output', \n",
    "                   F.concat(F.lit('The '), df.year, F.lit(' '), df.manufacturer, F.lit(' '), df.model, F.lit(' has a '), df.cyl, F.lit(' cylinder engine.')))\n",
    "\n",
    "# Transform the trans column to only contain either manual or auto\n",
    "df = df.withColumn('trans', F.when(df.trans.like('%manual%'), 'manual').otherwise('auto'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a9694-8811-4238-be07-2296dea54779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b1669a-2318-4b8f-a446-284d307d639a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m smokers_percentage \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(df\u001b[38;5;241m.\u001b[39msmoker \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m/\u001b[39m df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a column that contains the tip percentage\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtip_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtip\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_bill\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the average tip percentage for each combination of sex and smoker\u001b[39;00m\n\u001b[1;32m     14\u001b[0m avg_tip_percentage \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mavg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtip_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_tip_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the tips dataset from the pydataset library\n",
    "tips = data('tips')\n",
    "\n",
    "# Convert the pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(tips)\n",
    "\n",
    "# Calculate the percentage of observations that are smokers\n",
    "smokers_percentage = df.filter(df.smoker == 'Yes').count() / df.count() * 100\n",
    "\n",
    "# Create a column that contains the tip percentage\n",
    "df = df.withColumn('tip_percentage', F.col('tip') / F.col('total_bill') * 100)\n",
    "\n",
    "# Calculate the average tip percentage for each combination of sex and smoker\n",
    "avg_tip_percentage = df.groupby('sex', 'smoker').agg(F.avg('tip_percentage').alias('avg_tip_percentage'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(truncate=False)\n",
    "print(\"Percentage of observations that are smokers: {:.2f}%\".format(smokers_percentage))\n",
    "avg_tip_percentage.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f86d22-82e1-4c38-a735-2d33b6083272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b931dd29-35d0-48b5-a962-d707d314a1e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert temperatures to Fahrenheit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_fahrenheit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (weather[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_max\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m9\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display the updated dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# Convert temperatures to Fahrenheit\n",
    "df['temp_fahrenheit'] = (weather['temp_max'] * 9/5) + 32\n",
    "\n",
    "# Display the updated dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9d116-8d28-4960-ba62-46fd41a94e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
